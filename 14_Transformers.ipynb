{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a summarization dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train_data = dataset['train'].select(range(200))  # Keep it small for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_input=512, max_output=150):\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input, truncation=True)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=max_output, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " machine learning has transformed many industries . power of powerful neural network architectures such as transformers . models like BERT, GPT, and T5 have become industry standards .\n"
     ]
    }
   ],
   "source": [
    "custom_text = \"\"\"\n",
    "Machine learning has transformed many industries, from healthcare and finance to marketing and transportation. One of the key drivers of this transformation is the availability of powerful neural network architectures such as transformers. These models excel at understanding sequences, making them ideal for tasks like translation, summarization, and question answering. Among them, models like BERT, GPT, and T5 have become industry standards.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_text(custom_text)\n",
    "print(\"Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.4, 'rouge2': 0.15384615384615383, 'rougeL': 0.4, 'rougeLsum': 0.4}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "reference = \"Transformers are models that use attention to handle sequential data.\"\n",
    "prediction = \"Transformers use attention for sequences.\"\n",
    "\n",
    "results = rouge.compute(predictions=[prediction], references=[reference])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
